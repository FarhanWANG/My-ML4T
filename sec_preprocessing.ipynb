{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors from SEC filings using Gensim: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn word and phrase vectors from annual SEC filings using gensim to illustrate the potential value of word embeddings for algorithmic trading. In the following sections, we will combine these vectors as features with price returns to train neural networks to predict equity prices from the content of security filings.\n",
    "\n",
    "In particular, we use a dataset containing over 22,000 10-K annual reports from the period 2013-2016 that are filed by listed companies and contain both financial information and management commentary (see chapter 3 on Alternative Data). For about half of 11K filings for companies that we have stock prices to label the data for predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:44:28.754726Z",
     "start_time": "2020-06-21T16:44:28.749484Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:44:30.118307Z",
     "start_time": "2020-06-21T16:44:29.480278Z"
    }
   },
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import logging\n",
    "import spacy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:44:30.121155Z",
     "start_time": "2020-06-21T16:44:30.119389Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:44:30.133327Z",
     "start_time": "2020-06-21T16:44:30.122236Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f'{h:02.0f}:{m:02.0f}:{s:02.0f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:44:30.737737Z",
     "start_time": "2020-06-21T16:44:30.725561Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        filename='preprocessing.log',\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be downloaded from [here](https://drive.google.com/uc?id=0B4NK0q0tDtLFendmeHNsYzNVZ2M&export=download). Unzip and move into the `data` folder in the repository's root directory and rename to `filings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each filing is a separate text file and a master index contains filing metadata. We extract the most informative sections, namely\n",
    "- Item 1 and 1A: Business and Risk Factors\n",
    "- Item 7 and 7A: Management's Discussion and Disclosures about Market Risks\n",
    "\n",
    "The notebook preprocessing shows how to parse and tokenize the text using spaCy, similar to the approach in chapter 14. We do not lemmatize the tokens to preserve nuances of word usage.\n",
    "\n",
    "We use gensim to detect phrases. The Phrases module scores the tokens and the Phraser class transforms the text data accordingly. The notebook shows how to repeat the process to create longer phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:44:34.329500Z",
     "start_time": "2020-06-21T16:44:34.321563Z"
    }
   },
   "outputs": [],
   "source": [
    "sec_path = Path('.', 'data', 'sec-filings')\n",
    "filing_path = sec_path / 'filings'\n",
    "sections_path = sec_path / 'sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:44:34.912104Z",
     "start_time": "2020-06-21T16:44:34.903535Z"
    }
   },
   "outputs": [],
   "source": [
    "if not sections_path.exists():\n",
    "    sections_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T02:52:25.427816Z",
     "start_time": "2020-06-21T02:44:04.864855Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, filing in enumerate(filing_path.glob('*.txt'), 1):\n",
    "    if i % 500 == 0:\n",
    "        print(i, end=' ', flush=True)\n",
    "    filing_id = int(filing.stem)\n",
    "    items = {}\n",
    "    for section in filing.read_text().lower().split('Â°'):\n",
    "        if section.startswith('item '):\n",
    "            if len(section.split()) > 1:\n",
    "                item = section.split()[1].replace('.', '').replace(':', '').replace(',', '')\n",
    "                text = ' '.join([t for t in section.split()[2:]])\n",
    "                if items.get(item) is None or len(items.get(item)) < len(text):\n",
    "                    items[item] = text\n",
    "\n",
    "    txt = pd.Series(items).reset_index()\n",
    "    txt.columns = ['item', 'text']\n",
    "    txt.to_csv(sections_path / (filing.stem + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the following sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T02:52:25.430613Z",
     "start_time": "2020-06-21T02:52:25.428829Z"
    }
   },
   "outputs": [],
   "source": [
    "sections = ['1', '1a', '7', '7a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:45:03.290044Z",
     "start_time": "2020-06-21T16:45:03.287811Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_path = sec_path / 'selected_sections'\n",
    "if not clean_path.exists():\n",
    "    clean_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T03:24:54.421552Z",
     "start_time": "2020-06-21T03:24:53.708348Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-592faff8a0d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_core_web_sm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6000000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m def load(\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mdisable\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleFrozenList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleFrozenList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE095\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \"\"\"Initialize the frozen dict. Can be initialized with pre-defined\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(en_core_web_sm)\n",
    "nlp.max_length = 6000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T12:05:56.150635Z",
     "start_time": "2020-06-21T03:25:21.267371Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "t = total_tokens = 0\n",
    "stats = []\n",
    "\n",
    "start = time()\n",
    "to_do = len(list(sections_path.glob('*.csv')))\n",
    "done = len(list(clean_path.glob('*.csv'))) + 1\n",
    "for text_file in sections_path.glob('*.csv'):\n",
    "    file_id = int(text_file.stem)\n",
    "    clean_file = clean_path / f'{file_id}.csv'\n",
    "    if clean_file.exists():\n",
    "        continue\n",
    "    items = pd.read_csv(text_file).dropna()\n",
    "    items.item = items.item.astype(str)\n",
    "    items = items[items.item.isin(sections)]\n",
    "    if done % 100 == 0:\n",
    "        duration = time() - start\n",
    "        to_go = (to_do - done) * duration / done\n",
    "        print(f'{done:>5}\\t{format_time(duration)}\\t{total_tokens / duration:,.0f}\\t{format_time(to_go)}')\n",
    "    \n",
    "    clean_doc = []\n",
    "    for _, (item, text) in items.iterrows():\n",
    "        doc = nlp(text)\n",
    "        for s, sentence in enumerate(doc.sents):\n",
    "            clean_sentence = []\n",
    "            if sentence is not None:\n",
    "                for t, token in enumerate(sentence, 1):\n",
    "                    if not any([token.is_stop,\n",
    "                                token.is_digit,\n",
    "                                not token.is_alpha,\n",
    "                                token.is_punct,\n",
    "                                token.is_space,\n",
    "                                token.lemma_ == '-PRON-',\n",
    "                                token.pos_ in ['PUNCT', 'SYM', 'X']]):\n",
    "                        clean_sentence.append(token.text.lower())\n",
    "                total_tokens += t\n",
    "                if len(clean_sentence) > 0:\n",
    "                    clean_doc.append([item, s, ' '.join(clean_sentence)])\n",
    "    (pd.DataFrame(clean_doc,\n",
    "                  columns=['item', 'sentence', 'text'])\n",
    "     .dropna()\n",
    "     .to_csv(clean_file, index=False))\n",
    "    done += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:45:07.927900Z",
     "start_time": "2020-06-21T16:45:07.918296Z"
    }
   },
   "outputs": [],
   "source": [
    "ngram_path = sec_path / 'ngrams'\n",
    "stats_path = sec_path / 'corpus_stats'\n",
    "for path in [ngram_path, stats_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T13:32:42.332829Z",
     "start_time": "2020-06-21T13:32:42.331171Z"
    }
   },
   "outputs": [],
   "source": [
    "unigrams = ngram_path / 'ngrams_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T13:32:46.109338Z",
     "start_time": "2020-06-21T13:32:46.104347Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_unigrams(min_length=3):\n",
    "    texts = []\n",
    "    sentence_counter = Counter()\n",
    "    vocab = Counter()\n",
    "    for i, f in enumerate(clean_path.glob('*.csv')):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, end=' ', flush=True)\n",
    "        df = pd.read_csv(f)\n",
    "        df.item = df.item.astype(str)\n",
    "        df = df[df.item.isin(sections)]\n",
    "        sentence_counter.update(df.groupby('item').size().to_dict())\n",
    "        for sentence in df.text.dropna().str.split().tolist():\n",
    "            if len(sentence) >= min_length:\n",
    "                vocab.update(sentence)\n",
    "                texts.append(' '.join(sentence))\n",
    "    \n",
    "    (pd.DataFrame(sentence_counter.most_common(), \n",
    "                  columns=['item', 'sentences'])\n",
    "     .to_csv(stats_path / 'selected_sentences.csv', index=False))\n",
    "    (pd.DataFrame(vocab.most_common(), columns=['token', 'n'])\n",
    "     .to_csv(stats_path / 'sections_vocab.csv', index=False))\n",
    "    \n",
    "    unigrams.write_text('\\n'.join(texts))\n",
    "    return [l.split() for l in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T13:37:06.760726Z",
     "start_time": "2020-06-21T13:32:52.427226Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "if not unigrams.exists():\n",
    "    texts = create_unigrams()\n",
    "else:\n",
    "    texts = [l.split() for l in unigrams.open()]\n",
    "print('\\nReading: ', format_time(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T13:37:06.775365Z",
     "start_time": "2020-06-21T13:37:06.762138Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_ngrams(max_length=3):\n",
    "    \"\"\"Using gensim to create ngrams\"\"\"\n",
    "\n",
    "    n_grams = pd.DataFrame()\n",
    "    start = time()\n",
    "    for n in range(2, max_length + 1):\n",
    "        print(n, end=' ', flush=True)\n",
    "\n",
    "        sentences = LineSentence(ngram_path / f'ngrams_{n - 1}.txt')\n",
    "        phrases = Phrases(sentences=sentences,\n",
    "                          min_count=25,  # ignore terms with a lower count\n",
    "                          threshold=0.5,  # accept phrases with higher score\n",
    "                          max_vocab_size=40000000,  # prune of less common words to limit memory use\n",
    "                          delimiter=b'_',  # how to join ngram tokens\n",
    "                          progress_per=50000,  # log progress every\n",
    "                          scoring='npmi')\n",
    "\n",
    "        s = pd.DataFrame([[k.decode('utf-8'), v] for k, v in phrases.export_phrases(sentences)], \n",
    "                         columns=['phrase', 'score']).assign(length=n)\n",
    "\n",
    "        n_grams = pd.concat([n_grams, s])\n",
    "        grams = Phraser(phrases)\n",
    "        sentences = grams[sentences]\n",
    "        (ngram_path / f'ngrams_{n}.txt').write_text('\\n'.join([' '.join(s) for s in sentences]))\n",
    "\n",
    "    n_grams = n_grams.sort_values('score', ascending=False)\n",
    "    n_grams.phrase = n_grams.phrase.str.replace('_', ' ')\n",
    "    n_grams['ngram'] = n_grams.phrase.str.replace(' ', '_')\n",
    "\n",
    "    n_grams.to_parquet(sec_path / 'ngrams.parquet')\n",
    "\n",
    "    print('\\n\\tDuration: ', format_time(time() - start))\n",
    "    print('\\tngrams: {:,d}\\n'.format(len(n_grams)))\n",
    "    print(n_grams.groupby('length').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-21T13:33:01.712Z"
    }
   },
   "outputs": [],
   "source": [
    "create_ngrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:48:25.955300Z",
     "start_time": "2020-06-21T16:48:25.948615Z"
    }
   },
   "outputs": [],
   "source": [
    "percentiles=np.arange(.1, 1, .1).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.167784Z",
     "start_time": "2020-06-21T16:45:12.067631Z"
    }
   },
   "outputs": [],
   "source": [
    "nsents, ntokens = Counter(), Counter()\n",
    "for f in clean_path.glob('*.csv'):\n",
    "    df = pd.read_csv(f)\n",
    "    nsents.update({str(k): v for k, v in df.item.value_counts().to_dict().items()})\n",
    "    df['ntokens'] = df.text.str.split().str.len()\n",
    "    ntokens.update({str(k): v for k, v in df.groupby('item').ntokens.sum().to_dict().items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.173106Z",
     "start_time": "2020-06-21T16:47:22.168794Z"
    }
   },
   "outputs": [],
   "source": [
    "ntokens = pd.DataFrame(ntokens.most_common(), columns=['Item', '# Tokens'])\n",
    "nsents = pd.DataFrame(nsents.most_common(), columns=['Item', '# Sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.488178Z",
     "start_time": "2020-06-21T16:47:22.174110Z"
    }
   },
   "outputs": [],
   "source": [
    "nsents.set_index('Item').join(ntokens.set_index('Item')).plot.bar(secondary_y='# Tokens', rot=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.542194Z",
     "start_time": "2020-06-21T16:47:22.489506Z"
    }
   },
   "outputs": [],
   "source": [
    "ngrams = pd.read_parquet(sec_path / 'ngrams.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.543489Z",
     "start_time": "2020-06-21T16:45:19.211Z"
    }
   },
   "outputs": [],
   "source": [
    "ngrams.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.544028Z",
     "start_time": "2020-06-21T16:45:19.516Z"
    }
   },
   "outputs": [],
   "source": [
    "ngrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.544559Z",
     "start_time": "2020-06-21T16:45:20.162Z"
    }
   },
   "outputs": [],
   "source": [
    "ngrams.score.describe(percentiles=percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:47:22.545201Z",
     "start_time": "2020-06-21T16:45:23.624Z"
    }
   },
   "outputs": [],
   "source": [
    "ngrams[ngrams.score>.7].sort_values(['length', 'score']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:48:08.840933Z",
     "start_time": "2020-06-21T16:48:08.755568Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(stats_path / 'sections_vocab.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:48:09.531070Z",
     "start_time": "2020-06-21T16:48:09.500574Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:48:29.793631Z",
     "start_time": "2020-06-21T16:48:29.762029Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab.n.describe(percentiles).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:31.376347Z",
     "start_time": "2020-06-21T16:48:40.664233Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = Counter()\n",
    "for l in (ngram_path / 'ngrams_2.txt').open():\n",
    "    tokens.update(l.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:31.539973Z",
     "start_time": "2020-06-21T16:49:31.377290Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = pd.DataFrame(tokens.most_common(),\n",
    "                     columns=['token', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:31.564413Z",
     "start_time": "2020-06-21T16:49:31.541492Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:31.576141Z",
     "start_time": "2020-06-21T16:49:31.565627Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:31.713773Z",
     "start_time": "2020-06-21T16:49:31.576981Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens.loc[tokens.token.str.contains('_'), 'count'].describe(percentiles).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:31.814099Z",
     "start_time": "2020-06-21T16:49:31.714671Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens[tokens.token.str.contains('_')].head(20).to_csv(sec_path / 'ngram_examples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:31.906003Z",
     "start_time": "2020-06-21T16:49:31.815020Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens[tokens.token.str.contains('_')].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:43.419751Z",
     "start_time": "2020-06-21T16:49:43.410341Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:46.315225Z",
     "start_time": "2020-06-21T16:49:43.586015Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_FOLDER / 'assets.h5') as store:\n",
    "    prices = store['quandl/wiki/prices'].adj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:46.366663Z",
     "start_time": "2020-06-21T16:49:46.316232Z"
    }
   },
   "outputs": [],
   "source": [
    "sec = pd.read_csv(sec_path / 'filing_index.csv').rename(columns=str.lower)\n",
    "sec.date_filed = pd.to_datetime(sec.date_filed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:46.741667Z",
     "start_time": "2020-06-21T16:49:46.721940Z"
    }
   },
   "outputs": [],
   "source": [
    "sec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:49.396776Z",
     "start_time": "2020-06-21T16:49:49.390774Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:51.423980Z",
     "start_time": "2020-06-21T16:49:50.352212Z"
    }
   },
   "outputs": [],
   "source": [
    "first = sec.date_filed.min() + relativedelta(months=-1)\n",
    "last = sec.date_filed.max() + relativedelta(months=1)\n",
    "prices = (prices\n",
    "          .loc[idx[first:last, :]]\n",
    "          .unstack().resample('D')\n",
    "          .ffill()\n",
    "          .dropna(how='all', axis=1)\n",
    "          .filter(sec.ticker.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:53.694685Z",
     "start_time": "2020-06-21T16:49:51.425034Z"
    }
   },
   "outputs": [],
   "source": [
    "sec = sec.loc[sec.ticker.isin(prices.columns), ['ticker', 'date_filed']]\n",
    "\n",
    "price_data = []\n",
    "for ticker, date in sec.values.tolist():\n",
    "    target = date + relativedelta(months=1)\n",
    "    s = prices.loc[date: target, ticker]\n",
    "    price_data.append(s.iloc[-1] / s.iloc[0] - 1)\n",
    "\n",
    "df = pd.DataFrame(price_data,\n",
    "                  columns=['returns'],\n",
    "                  index=sec.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:53.700732Z",
     "start_time": "2020-06-21T16:49:53.695697Z"
    }
   },
   "outputs": [],
   "source": [
    "df.returns.describe()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:53.718447Z",
     "start_time": "2020-06-21T16:49:53.701618Z"
    }
   },
   "outputs": [],
   "source": [
    "sec['returns'] = price_data\n",
    "sec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:49:58.385354Z",
     "start_time": "2020-06-21T16:49:58.326570Z"
    }
   },
   "outputs": [],
   "source": [
    "sec.dropna().to_csv(sec_path / 'sec_returns.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
